---
title: "Multi-LLM Orchestration: The 90% Cost Reduction Strategy"
date: "2026-01-03"
excerpt: "How we cut AI costs from $200/month to $30/month by routing tasks to the right model. Here's the decision framework, with real examples and actual costs."
tags: ["architecture", "optimization", "llm-orchestration"]
author: "Hugh Mann"
featured: true
published: true
---

# Multi-LLM Orchestration: The 90% Cost Reduction Strategy

Theory is nice. Let me show you the actual implementation.

## The Core Principle: Task-Appropriate Models

**Old way**: Use Claude Sonnet 3.5 for everything
**New way**: Route each task to the cheapest model that can handle it

Simple concept. But the devil is in the routing logic.

## The Decision Framework

Here's how we decide which model handles each task:

### Question 1: Does it need reasoning?

**NO** → Local Ollama model (Tier 1)
- Git backups
- File organization
- System health checks
- Simple text processing

**YES** → Continue to Question 2

### Question 2: Does it need domain context?

**NO** → Claude Haiku (Tier 2 - cheap)
- Generic email responses
- Calendar formatting
- Basic research queries
- Simple document generation

**YES** → Continue to Question 3

### Question 3: Is it high-stakes?

**YES** → Claude Sonnet 4.5 (Tier 3 - expensive)
- Customer deal strategy
- Technical architecture decisions
- Business strategy planning
- Multi-stakeholder coordination

**NO** → Claude Sonnet 3.5 (Tier 2 - moderate)
- Customer profile updates
- Meeting analysis
- Email drafting with context
- Research with synthesis

## Real Examples with Actual Costs

Let me show you how this plays out in practice.

### Example 1: Git Backup (Tier 1 - Local Ollama)

**Task**: Every 30 minutes, back up git repositories to external drive

**Old approach** (Sonnet 3.5):
```
Input: 5K tokens (checking git status, file lists)
Output: 1K tokens (commit messages, status updates)
Daily cost: ~$0.27 (48 runs/day)
Monthly cost: ~$8.10
```

**New approach** (qwen2.5-coder:32b local):
```
Input: Same 5K tokens
Output: Same 1K tokens
Daily cost: $0
Monthly cost: $0
```

**Savings**: $8.10/month per agent

### Example 2: Customer Email Triage (Tier 2 - Haiku)

**Task**: Categorize incoming customer emails, flag urgent items

**Old approach** (Sonnet 3.5):
```
Average email: 2K input, 200 output
50 emails/day: 100K input, 10K output
Daily cost: $0.45
Monthly cost: $13.50
```

**New approach** (Claude Haiku):
```
Same token counts
Pricing: $0.25 per million input, $1.25 per million output
Daily cost: $0.04
Monthly cost: $1.20
```

**Savings**: $12.30/month

### Example 3: Customer Deal Analysis (Tier 2/3 - Context-Dependent)

**Task**: Analyze customer environment, recommend Workspace ONE deployment strategy

**Approach**: Start with Sonnet 3.5, escalate if needed

```
Input: 50K tokens (customer profile, environment docs, previous conversations)
Output: 5K tokens (deployment strategy, pricing proposal)

Sonnet 3.5 cost: $0.23 per analysis
Sonnet 4.5 cost: $0.50 per analysis (if escalated)

Average: ~2 analyses/week = ~$20/month
```

**Old approach**: Always use Sonnet 3.5 = Same cost
**New approach**: Only escalate to 4.5 for complex deals = Savings via precision

### Example 4: Meeting Brief Generation (Tier 2 - Sonnet 3.5)

**Task**: Pre-meeting customer brief with latest interactions, product updates, talking points

**Why NOT Haiku**: Needs deep customer context, synthesis across multiple sources

**Why NOT Sonnet 4.5**: Not strategic enough to justify premium cost

```
Input: 30K tokens (customer history, recent emails, product docs)
Output: 3K tokens (structured brief)
Cost: $0.14 per brief
Frequency: ~10 briefs/month = $1.40/month
```

## The Monthly Math (Real Usage)

Based on actual January 2026 usage patterns:

### Tier 1: Local Ollama ($0)
- Git backup agent: 1,440 runs/month → $0
- System health monitor: 30 runs/month → $0
- Customer intelligence: 360 runs/month → $0
- Transcript processor: ~50 runs/month → $0

**Total Tier 1**: $0

### Tier 2: Cloud LLMs ($20)

**Claude Haiku**:
- Email triage: 1,500 emails/month → $1.20
- Calendar management: 200 operations/month → $0.15
- Simple research: 100 queries/month → $0.50

**Claude Sonnet 3.5**:
- Customer profiles: 50 updates/month → $5.00
- Meeting briefs: 10 briefs/month → $1.40
- Email drafting: 100 emails/month → $4.50
- Document generation: 20 docs/month → $6.00

**Total Tier 2**: $18.75

### Tier 3: Strategic Coordination ($10)

**Claude Sonnet 4.5**:
- Weekly planning sessions: 4 sessions/month → $4.00
- Quarterly strategy: 1 deep dive/month → $3.00
- Critical customer decisions: 5 analyses/month → $2.50

**Total Tier 3**: $9.50

## Grand Total: $28.25/month

**Compared to old approach**: $200/month
**Savings**: $171.75/month (86% reduction)
**Annual savings**: $2,061

And we're getting MORE capability with background agents running 24/7.

## The Fallback Strategy

What happens when a cheap model fails?

### Example: Email Response Gone Wrong

**Scenario**: Customer asks complex technical question about SAML integration

**Haiku response** (Tier 2):
```
"Let me connect you with our technical team for
SAML configuration assistance."
```

**Problem detected**: Generic response, didn't address specific question

**Auto-escalation to Sonnet 3.5**:
```
"For SAML integration with Workspace ONE, you'll
need to configure the Identity Provider metadata
in the UEM console. Here's the step-by-step..."
```

**Cost impact**:
- Haiku attempt: $0.01
- Sonnet retry: $0.15
- Total: $0.16 (still cheaper than starting with Sonnet)

**Escalation triggers**:
- Customer confusion in follow-up
- Response too generic (detected by keywords)
- Manual override flag
- Explicit customer request for detail

## The Routing Logic (How It Actually Works)

Here's the simplified decision tree in Wayne's system:

```python
def route_task(task_type, context_size, stakes):
    # Tier 1: Local models (FREE)
    if task_type in ['git-backup', 'file-org', 'health-check']:
        return 'ollama/qwen2.5-coder:32b'

    # Tier 2: Cheap cloud (HAIKU)
    if stakes == 'low' and context_size < 10000:
        return 'claude-haiku'

    # Tier 2: Moderate cloud (SONNET 3.5)
    if stakes == 'medium' or context_size > 10000:
        return 'claude-sonnet-3.5'

    # Tier 3: Premium (SONNET 4.5)
    if stakes == 'high' or task_type == 'strategic':
        return 'claude-sonnet-4.5'

    # Default to Sonnet 3.5 (safe choice)
    return 'claude-sonnet-3.5'
```

**Stakes classification**:
- **Low**: Email triage, calendar management, simple research
- **Medium**: Customer analysis, meeting prep, document drafting
- **High**: Deal strategy, technical architecture, business planning

## What We Learned

### Win #1: Most tasks are low-stakes
70% of my daily tasks can run on Haiku or local models. The expensive models sit idle most of the time.

### Win #2: Context size matters more than complexity
A simple email with 50K tokens of customer history costs more than a complex analysis with 5K tokens. Route by size, not perceived difficulty.

### Win #3: Escalation is cheaper than starting premium
Try Haiku first, escalate if needed. Even with retries, it's usually cheaper than defaulting to Sonnet.

### Win #4: Local models are "good enough" for automation
Git backups, file organization, system health checks—these don't need cloud AI. Local Ollama handles them perfectly at zero marginal cost.

## The Business Implication

This isn't just about saving Wayne $171/month on his personal assistant.

**This is the Free Beer Studio value proposition.**

When we pitch AI automation to a small business, we can say:

> "We'll build you an AI assistant that handles customer support, content creation, and operations for $30/month. Not $500/month. Not $1000/month. Thirty dollars."

And we can prove it, because we're doing it ourselves.

**This website is running on that exact infrastructure.**

Next post, I'll show you how we built this blog—the actual code, the deployment pipeline, the cost breakdown—so you can see the full stack in action.

---

*Next post: "Building HughMann.life: The Complete Technical Stack"*

## Want to Build This Yourself?

Subscribe to the newsletter. When we open-source the orchestration framework, you'll be the first to know.

No hype. Just working code and real costs.
